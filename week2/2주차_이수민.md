# 3-Buckets of Tooling
![3-buckets of tooling](https://soomin-study.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F7e09df14-435f-472c-9d77-770f53521511%2FUntitled.png?table=block&id=d83c051a-472d-4737-8e01-55658aa66bc6&spaceId=38af91ff-3978-4806-8675-534f3673b8c6&width=2000&userId=&cache=v2)
*From fullstackdeeplearning.com*
## 1. Software Engineering
### Language: Python
- libraries for data/ML ex. Pandas, Numpy, Scikit-learn
- low level(C, C++) code → python Wrapper → python code

### IDE
1. VS Code
- built-in git, open remote projects, notebook port forwarding
2. Jupyter Notebooks
- quick prototyping, exploratory analysis
	- Good: write code, test, documentations in 1 platform
	- Bad: ["5 reasons why jupyter notebooks suck"](https://towardsdatascience.com/5-reasons-why-jupyter-notebooks-suck-4dc201e27086)
3. Streamlit
- Backend code를 따로 쓰지 않고도 application을 만들어 확인해볼 수 있는 Python Framework
![img](https://soomin-study.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F16ec4d34-4e87-4107-97b9-aeabb54080b4%2F%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2022-03-24_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_12.29.24.png?table=block&id=623299a0-63ea-4706-a160-5facdf86deb6&spaceId=38af91ff-3978-4806-8675-534f3673b8c6&width=2000&userId=&cache=v2)


## 2. Computer Hardware
- model parameter 수↑ → compute 속도가 중요해짐
- your own hardware / cloud / on-premise?
	- GPU
		- NVIDIA GPU vs. Google TPU? → TPU가 더 빠르지만, GCP에서만 쓸 수 있음.
		- NVIDIA architecture: Kepler → Pascal → Volta → Turing → Ampere 순으로 새로 나옴
		- RAM↑ → large batches
	- Cloud Options: AWS, GCP, Microsoft Azure + Lambda Labs, Corewave

## 3. Resource Management
- python script에 직접 명시
- Slurm workload manager
- Docker + Kubernetes
- Custom ML software e.g. AWS Sagemaker

## 4. Framworks & Distributed training
### Framework
- TF/keras와 Pytorch 비교: [github](https://github.com/KerasKorea/KEKOxTutorial/blob/master/42_keras_or_pytorch_as_your_first_deep_learning_framework.md) 참고.

### Distributed Training
- model 하나에 GPU를 여러 개 쓰는 것
- data parallelism은 추천하지만, model parallelism의 경우 complexity가 올라가니 쓰지 않는 것이 좋다.

## 5. Experiment Management
- 여러 번의 실험결과를 자동으로 tracking하여 쉽게 monitor할 수 있다.
- Tensorboard, MLFlow, Comet.ml, Weight and Biases

## 6. Hyperparameter Tuning
- autoML
- optimal hyperparameter를 찾을 수 있도록 도와주는 tool: SigOpt, Ray Tune, Weight and Biases

## 7. All-in-one Solution
- Facebook's FBLearner, Google Cloud AI Platform, AWS SageMaker